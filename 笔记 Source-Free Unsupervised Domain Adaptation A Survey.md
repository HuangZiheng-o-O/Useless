###### Abstract

Unsupervised domain adaptation (UDA) via deep learning has attracted appealing attention for tackling domain-shift problems caused by distribution discrepancy across different domains. Existing UDA approaches highly depend on the accessibility of source domain data, which is usually limited in practical scenarios due to privacy protection, data storage and transmission cost, and computation burden. To tackle this issue, many source-free unsupervised domain adaptation (SFUDA) methods have been proposed recently, which perform knowledge transfer from a pre-trained source model to unlabeled target domain with source data inaccessible. A comprehensive review of these works on SFUDA is of great significance. In this paper, we provide a timely and systematic literature review of existing SFUDA approaches from a technical perspective. Specifically, we categorize current SFUDA studies into two groups, i.e., white-box SFUDA and black-box SFUDA, and further divide them into finer subcategories based on different learning strategies they use. We also investigate the challenges of methods in each subcategory, discuss the advantages/disadvantages of white-box and black-box SFUDA methods, conclude the commonly used benchmark datasets, and summarize the popular techniques for improved generalizability of models learned without using source data. We finally discuss several promising future directions in this field.

###### Index Terms:

Domain adaptation, source-free, unsupervised learning, survey.



Intra-domain patch-level self-supervision module (IPSM) Intra-domain patch-level self-supervision module (IPSM) çš„ç›®çš„æ˜¯ä¸ºäº†åœ¨æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰ä¸­æé«˜ä¼ªæ ‡ç­¾çš„åˆ©ç”¨ç‡ï¼Œå°¤å…¶æ˜¯åœ¨æƒ…æ™¯æ„ŸçŸ¥åˆ†å‰²ä»»åŠ¡ä¸­ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤§åŸŸé—´éš”æ¡ä»¶ä¸‹åˆ†ç¦»ç®€å•å’Œå›°éš¾æ ·æœ¬çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡ç†µåŸºæ’åºæ¥å°†ç›®æ ‡åŸŸçš„æ ·æœ¬åˆ†ä¸ºæ˜“åˆ†ç»„å’Œéš¾åˆ†ç»„ï¼Œå¹¶ä½¿ç”¨ä¸€ç§å¯¹æŠ—æœºåˆ¶æ¥å‡å°‘åŸŸé—´æˆ–åŸŸå†…çš„å·®è·ã€‚ä¸‹é¢è¯¦ç»†è§£é‡Š IPSM çš„å·¥ä½œæœºåˆ¶ï¼š

1. **ç”Ÿæˆç†µå›¾ (Entropy Maps) å’Œç‰¹å¾å›¾ (Feature Maps)**:
   - å¯¹äºç»™å®šçš„ç›®æ ‡åŸŸæ•°æ®ï¼Œæ¨¡å‹é¦–å…ˆç”Ÿæˆç‰¹å¾å›¾ã€‚
   - æ¯ä¸ªç‰¹å¾å›¾å¯¹åº”çš„ç†µå›¾é€šè¿‡å¯¹æ¨¡å‹é¢„æµ‹è¾“å‡ºåº”ç”¨ softmax å‡½æ•°å’Œåç»­å¤„ç†è®¡ç®—å¾—åˆ°ã€‚ç†µå›¾ä¸­æ¯ä¸ªåƒç´ çš„ç†µå€¼è¡¨ç¤ºè¯¥åƒç´ åˆ†ç±»çš„ä¸ç¡®å®šæ€§ï¼Œç†µå€¼è¶Šé«˜è¡¨ç¤ºä¸ç¡®å®šæ€§è¶Šå¤§ã€‚

2. **ç†µæ’åº (Entropy Ranking)**:
   - ä½¿ç”¨ç†µå€¼å°†æ¯ä¸ªæ ·æœ¬ï¼ˆå›¾åƒï¼‰çš„ç†µå›¾åˆ†å‰²æˆå¤šä¸ªå°å—æˆ–è€…è¡¥ä¸ã€‚
   - è¿™äº›è¡¥ä¸æ ¹æ®å…¶ä½ç½®è¢«åˆ†ç±»ä¸º KÃ—K ç±»åˆ«ã€‚
   - é€šè¿‡è®¡ç®—æ¯ä¸ªè¡¥ä¸çš„å¹³å‡ç†µå€¼ï¼Œå¯ä»¥å°†è¡¥ä¸åˆ†ä¸ºâ€œç®€å•â€å’Œâ€œå›°éš¾â€ä¸¤ç»„ï¼Œç®€å•çš„è¡¥ä¸å…·æœ‰è¾ƒä½çš„å¹³å‡ç†µå€¼ï¼Œè€Œå›°éš¾çš„è¡¥ä¸åˆ™ç›¸åã€‚

3. **è¡¥ä¸çº§è‡ªæˆ‘ç›‘ç£ (Patch-level Self-supervision)**:
   - å¯¹äºç®€å•å’Œå›°éš¾çš„è¡¥ä¸ï¼Œåˆ†åˆ«ç”Ÿæˆé¢„æµ‹å›¾ \( I_t^o \) å’Œ \( I_t^' \)ã€‚
   - ç„¶åè®­ç»ƒä¸€ä¸ªåˆ¤åˆ«å™¨ \( D \)ï¼Œå®ƒé€šè¿‡å­¦ä¹ åŒºåˆ†ç®€å•å’Œå›°éš¾çš„è¡¥ä¸æ¥å‡å°‘å®ƒä»¬ä¹‹é—´çš„å·®è·ã€‚è¿™ä¸€æ­¥ç§°ä¸ºå¯¹æŠ—æ€§è®­ç»ƒ (adversarial learning)ã€‚

4. **å¯¹æŠ—æ€§æŸå¤± (ADV loss)**:
   - åˆ¤åˆ«å™¨ \( D \) çš„ç›®çš„æ˜¯æœ€å¤§åŒ–ç®€å•è¡¥ä¸ä¸å›°éš¾è¡¥ä¸åœ¨åˆ¤åˆ«å™¨è¾“å‡ºä¸­çš„å·®å¼‚ã€‚
   - ä½¿ç”¨å¯¹æŠ—æ€§æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°åŒºåˆ†è¿™ä¸¤ç±»è¡¥ä¸ï¼Œå¹¶å› æ­¤å‡å°‘å®ƒä»¬ä¹‹é—´çš„å·®è·ã€‚

5. **å…¬å¼ (1), (13), (14), (15) çš„è§£é‡Š**:
   - å…¬å¼ (13) æè¿°äº†å¦‚ä½•è®¡ç®—ç›®æ ‡å›¾åƒè¡¥ä¸çš„å¹³å‡ç†µå€¼ã€‚
   - å…¬å¼ (14) è¡¨ç¤ºç†µæ’åºçš„è¿‡ç¨‹ï¼Œå…¶ä¸­ \( Rank(\cdot) \) æ˜¯åŸºäºç†µå€¼çš„æ’åºå‡½æ•°ã€‚
   - å…¬å¼ (15) å®šä¹‰äº†å¯¹æŠ—æ€§æŸå¤±ï¼Œå®ƒç”¨äºè®­ç»ƒåˆ¤åˆ«å™¨ \( D \) ä»¥åŒºåˆ†ç®€å•å’Œå›°éš¾çš„è¡¥ä¸ã€‚è¿™ä¸ªæŸå¤±é¼“åŠ±æ¨¡å‹å°†ç®€å•è¡¥ä¸çš„é¢„æµ‹åå‘äºçœŸå®æ ‡ç­¾ï¼Œè€Œå°†å›°éš¾è¡¥ä¸çš„é¢„æµ‹åå‘äºä¸ç®€å•è¡¥ä¸é¢„æµ‹ä¸åŒçš„æ–¹å‘ã€‚

é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å¯ç”¨æºåŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ç›®æ ‡åŸŸå†…çš„æ•°æ®è‡ªæˆ‘ç›‘ç£ï¼Œä»¥å‡å°‘ç›®æ ‡åŸŸå†…éƒ¨çš„æ ·æœ¬å·®å¼‚ï¼Œå¹¶æé«˜UDAä»»åŠ¡ä¸­æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚

```
Instead of generating source-like images directly, some studies propose to align feature
prototypes or feature distribution of source data [40, 41, 42, 43, 44] with those in the target domain. Specifically, Qiu et al. [40] generate feature prototypes for each source category based on a conditional generator and produce pseudo-labels for the target data. The
cross-domain prototype adaptation is achieved by aligning the features derived from
pseudo-labeled target samples to source prototype with the same category label via contrastive learning. Tian et al. [41] construct a virtual domain by simply sampling from an
approximated gaussian mixture model (GMM) to mimic unseen source domain distribution. In terms of adaptation procedure, they reduce the distribution gap between the constructed virtual domain and the target domain via adversarial training, thus bypassing inaccessible source domain. Their practice is based on the assumption that the feature prototype of each category can be mined from each row of the source classifierâ€™ weights [45].
With the same assumption, Ding et al. [42] leverage such source classifier weights and reliable target pseudo-labels derived by spherical k-means clustering to estimate source feature distribution. After that, proxy source data can be sampled from the estimated source
distribution, and a conventional domain adaptation strategy [46] is used to explicitly perform cross-domain feature distribution alignment. Stan et al. [43, 44] propose to first generate a prototypical distribution representing the source data in an embedding feature
space via GMM, and then perform source-free adaptation by enforcing distribution alignment between source and target domains via sliced Wasserstein distance [47].

ä»€ä¹ˆæ˜¯è¿‘ä¼¼â¾¼æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ 
â¾¸å…ˆé€šè¿‡GMMâ½£æˆâ¼€ä¸ªåŸå‹åˆ†å¸ƒï¼Œ è¡¨
â½°åµŒâ¼Šç‰¹å¾ç©ºé—´ä¸­çš„æºæ•°æ®ï¼Œ ç„¶åé€šè¿‡åˆ‡â½šWassersteinè·ç¦»å¼ºåˆ¶æºåŸŸå’Œâ½¬ æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒå¯¹é½æ¥
æ‰§â¾â½†æºé€‚åº”[ 47]ã€‚

è¿™æ˜¯å•¥æ„æ€

```

è¿™æ®µå†…å®¹è®¨è®ºçš„æ˜¯åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢†åŸŸé€‚åº”ï¼ˆdomain adaptationï¼‰é—®é¢˜ä¸Šçš„ä¸€äº›ç ”ç©¶æ–¹æ³•ã€‚åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹ï¼Œé¢†åŸŸé€‚åº”æ˜¯æŒ‡å¦‚ä½•è°ƒæ•´æˆ–è®­ç»ƒä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿä»ä¸€ä¸ªé¢†åŸŸï¼ˆæºåŸŸï¼‰è¿ç§»åˆ°å¦ä¸€ä¸ªä¸åŒçš„é¢†åŸŸï¼ˆç›®æ ‡åŸŸï¼‰ï¼Œå³ä½¿è¿™ä¸¤ä¸ªé¢†åŸŸçš„æ•°æ®åˆ†å¸ƒå¯èƒ½ä¸åŒã€‚è¿™ç§æƒ…å†µåœ¨å®é™…åº”ç”¨ä¸­éå¸¸å¸¸è§ï¼Œæ¯”å¦‚ä¸€ä¸ªæ¨¡å‹åœ¨ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ä¸Šè®­ç»ƒï¼Œç„¶åå¸Œæœ›å°†å…¶åº”ç”¨åˆ°å¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ä¸Šã€‚

å…·ä½“åˆ°ä½ æåˆ°çš„ç ”ç©¶æ–¹æ³•ï¼š

1. **ç‰¹å¾åŸå‹ï¼ˆFeature Prototypesï¼‰ä¸ç‰¹å¾åˆ†å¸ƒçš„å¯¹é½**ï¼šä¸€äº›ç ”ç©¶æå‡ºï¼Œä¸ç›´æ¥ç”Ÿæˆä¸æºæ•°æ®ç›¸ä¼¼çš„å›¾åƒï¼Œè€Œæ˜¯é€šè¿‡å¯¹é½æºæ•°æ®çš„ç‰¹å¾åŸå‹æˆ–ç‰¹å¾åˆ†å¸ƒä¸ç›®æ ‡åŸŸçš„ç‰¹å¾æ¥å®ç°é¢†åŸŸé€‚åº”ã€‚ç‰¹å¾åŸå‹å¯ä»¥è§†ä¸ºæŸä¸€ç±»åˆ«æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºçš„å…¸å‹æˆ–å¹³å‡å½¢æ€ã€‚

2. **Qiu et al.**ï¼šè¿™é¡¹ç ”ç©¶é€šè¿‡æ¡ä»¶ç”Ÿæˆå™¨ä¸ºæ¯ä¸ªæºç±»åˆ«ç”Ÿæˆç‰¹å¾åŸå‹ï¼Œå¹¶ä¸ºç›®æ ‡æ•°æ®äº§ç”Ÿä¼ªæ ‡ç­¾ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå°†ä»ä¼ªæ ‡ç­¾ç›®æ ‡æ ·æœ¬ä¸­å¾—åˆ°çš„ç‰¹å¾ä¸ç›¸åŒç±»åˆ«çš„æºåŸå‹å¯¹é½ï¼Œä»è€Œå®ç°è·¨åŸŸåŸå‹é€‚åº”ã€‚

3. **Tian et al.**ï¼šé€šè¿‡ä»ä¸€ä¸ªè¿‘ä¼¼çš„é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰é‡‡æ ·æ¥æ„å»ºä¸€ä¸ªè™šæ‹ŸåŸŸï¼Œæ¨¡ä»¿æœªè§è¿‡çš„æºåŸŸåˆ†å¸ƒã€‚ä»–ä»¬é€šè¿‡å¯¹æŠ—è®­ç»ƒå‡å°‘æ„å»ºçš„è™šæ‹ŸåŸŸä¸ç›®æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒå·®è·ï¼Œä»è€Œç»•è¿‡æ— æ³•è®¿é—®çš„æºåŸŸã€‚

4. **GMMï¼ˆé«˜æ–¯æ··åˆæ¨¡å‹ï¼‰**ï¼šGMMæ˜¯ä¸€ç§ç»Ÿè®¡æ¨¡å‹ï¼Œç”¨äºè¡¨ç¤ºå…·æœ‰å¤šä¸ªå­ç¾¤ä½“çš„æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªå­ç¾¤ä½“çš„æ•°æ®åˆ†å¸ƒå¯ä»¥ç”¨é«˜æ–¯åˆ†å¸ƒï¼ˆå³æ­£æ€åˆ†å¸ƒï¼‰æ¥è¿‘ä¼¼ã€‚åœ¨è¿™é‡Œï¼Œå®ƒè¢«ç”¨æ¥ç”Ÿæˆä¸€ä¸ªåŸå‹åˆ†å¸ƒï¼Œä»£è¡¨åµŒå…¥ç‰¹å¾ç©ºé—´ä¸­çš„æºæ•°æ®ã€‚

5. **é€šè¿‡åˆ‡ç‰‡Wassersteinè·ç¦»è¿›è¡Œçš„æ— æºé€‚åº”**ï¼šStan et al. æå‡ºé¦–å…ˆé€šè¿‡GMMç”Ÿæˆä»£è¡¨æºæ•°æ®çš„åŸå‹åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨åˆ‡ç‰‡Wassersteinè·ç¦»æ¥å¼ºåˆ¶æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒå¯¹é½ï¼Œå®ç°æ— æºé€‚åº”ã€‚åˆ‡ç‰‡Wassersteinè·ç¦»æ˜¯ä¸€ç§åº¦é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºé«˜ç»´æ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥åœ¨ä¸ç›´æ¥è®¿é—®æºæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè°ƒæ•´æ¨¡å‹ä»¥æ›´å¥½åœ°é€‚åº”ç›®æ ‡åŸŸã€‚

è¿™äº›æ–¹æ³•éƒ½æ˜¯ä¸ºäº†è§£å†³å½“æºæ•°æ®ä¸å¯è®¿é—®æˆ–å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°ä¸åŒä½†ç›¸å…³çš„åŸŸæ—¶çš„é¢†åŸŸé€‚åº”é—®é¢˜ã€‚

2.2.1 Self-Supervised Knowledge Distillation
2.2.1â¾ƒç›‘ç£çŸ¥è¯†è’¸é¦

```
è¯¦ç»†è§£é‡ŠEMA 
Aug-ğ›¼ å’Œ Aug- ğ›½ 
Lğ¾ğ·
```

åœ¨è¿™å¼ å›¾ç‰‡ä¸­ï¼Œæè¿°çš„æ˜¯ä¸€ä¸ªç”¨äºæºæ— ç›‘ç£åŸŸé€‚åº”çš„è‡ªæˆ‘ç›‘ç£çŸ¥è¯†è’¸é¦æ–¹æ³•ã€‚ä¸‹é¢è¯¦ç»†è§£é‡Šå›¾ä¸­çš„æ¯ä¸ªéƒ¨åˆ†ï¼š

1. **EMA (Exponential Moving Average)**:
   EMAé€šå¸¸ç”¨äºå¹³æ»‘æ—¶é—´åºåˆ—æ•°æ®ï¼Œä»¥æ­¤æ¥è·Ÿè¸ªå˜é‡çš„è¶‹åŠ¿ã€‚åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼ŒEMAå¯ä»¥ç”¨äºç¨³å®šæ¨¡å‹è®­ç»ƒï¼Œé€šè¿‡å¯¹æ¨¡å‹å‚æ•°çš„ä¸€ä¸ªç‰ˆæœ¬è¿›è¡Œå¹³å‡ã€‚åœ¨æ•™å¸ˆç½‘ç»œï¼ˆTeacher Networkï¼‰ä¸­ï¼Œå‚æ•°é€šå¸¸ä¼šä½¿ç”¨å…¶è‡ªèº«å’Œå­¦ç”Ÿç½‘ç»œï¼ˆStudent Networkï¼‰å‚æ•°çš„EMAæ¥æ›´æ–°ã€‚è¿™æœ‰åŠ©äºåœ¨è¿ç§»å­¦ä¹ è¿‡ç¨‹ä¸­æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

   [ä»€ä¹ˆæ˜¯EMAï¼Ÿå¦‚ä½•ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿å’Œå…¬å¼ --- What is EMA? How to Use Exponential Moving Average With Formula (investopedia.com)](https://www.investopedia.com/terms/e/ema.asp)

2. **Aug-Î± å’Œ Aug-Î²**:
   è¿™ä¸¤ä¸ªæœ¯è¯­è¡¨ç¤ºæ•°æ®å¢å¼ºæ–¹æ³•ã€‚Aug-Î±å’ŒAug-Î²åˆ†åˆ«ä»£è¡¨ä¸¤ç§ä¸åŒçš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¾‹å¦‚å›¾åƒçš„ç¿»è½¬ã€æ—‹è½¬ã€å¹³ç§»ã€å™ªå£°æ·»åŠ ã€æ‰­æ›²ç­‰ã€‚æ•°æ®å¢å¼ºæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå¯ä»¥é€šè¿‡åˆ›é€ æ•°æ®çš„å˜ä½“æ¥æ‰©å……è®­ç»ƒé›†ï¼Œæœ‰åŠ©äºæ¨¡å‹å­¦ä¹ æ›´ä¸ºé²æ£’çš„ç‰¹å¾ï¼Œå¹¶é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

3. **LKD (Knowledge Distillation Loss)**:
   çŸ¥è¯†è’¸é¦æŸå¤±æ˜¯ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œå…¶ä¸­ä¸€ä¸ªè¾ƒå°çš„å­¦ç”Ÿç½‘ç»œè¢«è®­ç»ƒä»¥æ¨¡ä»¿ä¸€ä¸ªè¾ƒå¤§çš„æ•™å¸ˆç½‘ç»œçš„è¡Œä¸ºã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå­¦ç”Ÿç½‘ç»œè¯•å›¾å­¦ä¹ æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºæˆ–å…¶æŸäº›ä¸­é—´è¡¨ç¤ºã€‚LKDæ˜¯åœ¨å­¦ç”Ÿç½‘ç»œçš„è¾“å‡ºå’Œæ•™å¸ˆç½‘ç»œçš„è¾“å‡ºä¹‹é—´è®¡ç®—çš„æŸå¤±å‡½æ•°ï¼Œç›®æ ‡æ˜¯ä½¿å­¦ç”Ÿç½‘ç»œçš„é¢„æµ‹å°½å¯èƒ½æ¥è¿‘äºæ•™å¸ˆç½‘ç»œçš„é¢„æµ‹ã€‚

å›¾ç‰‡ç»“æ„è¯´æ˜ï¼š

- å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªè‡ªæˆ‘ç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨æ²¡æœ‰æºåŸŸæ•°æ®çš„æƒ…å†µä¸‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚
- ç›®æ ‡æ•°æ® \( X_T \) ä»£è¡¨ç›®æ ‡åŸŸä¸­çš„æ•°æ®é›†ã€‚
- ç›®æ ‡æ•°æ®é€šè¿‡ Aug-Î± å’Œ Aug-Î² è¿™ä¸¤ç§ä¸åŒçš„æ•°æ®å¢å¼ºæ–¹æ³•è¿›è¡Œå¢å¼ºå¤„ç†ï¼Œäº§ç”Ÿå˜ä½“ã€‚
- å­¦ç”Ÿç½‘ç»œï¼ˆStudent Networkï¼‰å’Œæ•™å¸ˆç½‘ç»œï¼ˆTeacher Networkï¼‰éƒ½æ˜¯ç”¨æ¥å¤„ç†è¿™äº›å¢å¼ºçš„ç›®æ ‡æ•°æ®ã€‚
- æ•™å¸ˆç½‘ç»œçš„å‚æ•°é€šè¿‡å¯¹å­¦ç”Ÿç½‘ç»œçš„å‚æ•°è¿›è¡ŒEMAå¤„ç†æ¥æ›´æ–°ã€‚
- å­¦ç”Ÿç½‘ç»œé€šè¿‡æœ€å°åŒ–çŸ¥è¯†è’¸é¦æŸå¤± LKD æ¥è®­ç»ƒï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°è¡¡é‡çš„æ˜¯å­¦ç”Ÿç½‘ç»œçš„è¾“å‡ºå’Œç»è¿‡EMAå¤„ç†çš„æ•™å¸ˆç½‘ç»œè¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚

æ•´ä¸ªè¿‡ç¨‹ç›®çš„æ˜¯æ— éœ€æºåŸŸæ•°æ®å°±èƒ½è®©æ¨¡å‹é€‚åº”ç›®æ ‡åŸŸï¼Œè¿™å¯¹äºåœ¨æ•°æ®è®¿é—®å—é™æˆ–éšç§é—®é¢˜æ˜¾è‘—çš„æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ã€‚é€šè¿‡è¿™ç§è‡ªæˆ‘ç›‘ç£çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ä»ç›®æ ‡æ•°æ®ä¸­å­¦ä¹ å¹¶æé«˜å…¶åœ¨ç›®æ ‡åŸŸçš„æ€§èƒ½ã€‚

```
Many studies [50, 51, 52, 23, 53, 54, 55, 56] transfer knowledge learned from source data to
the target model via knowledge distillation in a self-supervised manner, as illustrated in
Fig. 6. In these works, most of them [50, 51, 52, 23, 53] achieve source-free domain adaptation via a mean-teacher scheme for knowledge transfer [57], where the target model not
only learns from unseen target domain but also well preserves source model information.
For instance, Liu et al. [50] propose a self-supervised distillation scheme for automatic
polyp detection. By means of keeping output consistency of weak and strong augmented
polyp images, source knowledge is implicitly transferred to the target model with a mean
teacher strategy [57]. Besides, a diversification flow paradigm is designed to gradually
eliminate the style sensitivity among different domains, further enhancing model robustness towards style diversification. Yang et al. [51] also propose a self-supervised meanteacher approach for knowledge distillation, with a Transformer [58] embedded. This
helps the target model focus on object regions rather than less informative background in
an image, thus improving model generalizability. Assuming that both source and target
images are generated from a domain-invariant space by adding noise perturbations on
each specific domain, Xiong et al. [52] establish a super target domain via augmenting perturbations based on original target domain. The super and original target domains are fed
into a mean-teacher framework, with three consistency regularization terms (w.r.t. image,
instance, and class-wise) introduced for domain alignment. Chen et al. [23] first divide the
target data into clean and noisy subsets guided by a computation loss and regard them as
labeled and unlabeled examples, and then utilize the mean teacher technique to self-generate pseudo-labels for the unlabeled target data for domain adaptation.


ä½ ä»¥å‰çš„è§£é‡Šå¤ªä¸è¯¦ç»† è¯¥æœ‰çš„ç†è§£å’Œå…¬å¼éƒ½æ²¡è¯´ å¼€å§‹å®Œæˆï¼š
mean-teacher scheme æ˜¯å•¥ è¯¦ç»†è§£é‡Š
diversification flow paradigmæ˜¯å•¥ è¯¦ç»†è§£é‡Š

```



éå¸¸å¥½ï¼Œæˆ‘ä»¬å¯ä»¥é€ä¸€æ·±å…¥æ¢è®¨è¿™äº›ç ”ç©¶å·¥ä½œåŠå…¶å¯¹åŸŸé€‚åº”é—®é¢˜çš„ä¸åŒæ–¹æ³•ã€‚

1. **Liu et al. [53] çš„å¤šæ•™å¸ˆå¤šå­¦ç”Ÿæ¡†æ¶**ï¼š
   å¸¸è§„çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ä¸€èˆ¬ä½¿ç”¨ä¸€ä¸ªæ•™å¸ˆç½‘ç»œæŒ‡å¯¼ä¸€ä¸ªå­¦ç”Ÿç½‘ç»œã€‚è€ŒLiuç­‰äººæå‡ºäº†ä¸€ä¸ªå¤šæ•™å¸ˆå¤šå­¦ç”Ÿçš„æ¡†æ¶ï¼Œè¿™é‡Œçš„**æ¯ä¸ªæ•™å¸ˆ/å­¦ç”Ÿç½‘ç»œéƒ½æ˜¯ç”¨é¢„è®­ç»ƒçš„å…¬å…±ç½‘ç»œåˆå§‹åŒ–çš„**ï¼Œè¿™æ ·çš„ç½‘ç»œå¯èƒ½æ˜¯åœ¨å•ä¸ªæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæ„é€ äº†ä¸€ä¸ª**å›¾æ¨¡å‹æ¥è¡¨ç¤ºæ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§**ï¼Œå¹¶ä¸”æ•™å¸ˆç½‘ç»œé¢„æµ‹çš„è¿™ç§ç›¸ä¼¼å…³ç³»ç”¨äºé€šè¿‡å¹³å‡æ•™å¸ˆæŠ€æœ¯æŒ‡å¯¼å­¦ç”Ÿç½‘ç»œã€‚è¿™ç§æ–¹æ³•çš„å…³é”®åœ¨äºï¼Œå®ƒ**åˆ©ç”¨äº†å¤šä¸ªè§†è§’ï¼ˆæ¥è‡ªä¸åŒæ•™å¸ˆç½‘ç»œçš„ï¼‰**æ¥æä¾›ä¸€ä¸ªæ›´å…¨é¢å’Œé²æ£’çš„çŸ¥è¯†åŸºç¡€ï¼ŒåŒæ—¶å­¦ç”Ÿç½‘ç»œä»è¿™äº›æ•™å¸ˆç½‘ç»œä¸­è·å¾—æ›´ä¸ºä¸°å¯Œçš„ä¿¡æ¯ã€‚

2. **Yu et al. [54] çš„é£æ ¼å’Œç»“æ„è§„èŒƒåŒ–ä»¥åŠç‰©ç†å…ˆéªŒçº¦æŸ**ï¼š
   Yuç­‰äººæå‡ºçš„æ–¹æ³•ä¸ä¼ ç»Ÿçš„å¹³å‡æ•™å¸ˆèŒƒå¼ä¸åŒã€‚åœ¨ä»–ä»¬çš„å·¥ä½œä¸­ï¼Œä¸æ˜¯é€šè¿‡å¹³å‡å­¦ç”Ÿç½‘ç»œçš„æƒé‡æ¥è’¸é¦çŸ¥è¯†ï¼Œè€Œæ˜¯é€šè¿‡**é£æ ¼å’Œç»“æ„è§„èŒƒåŒ–ä»¥åŠç‰©ç†å…ˆéªŒçº¦æŸæ¥ä»æ•™å¸ˆç½‘ç»œå‘å­¦ç”Ÿç½‘ç»œè½¬ç§»çŸ¥è¯†**ã€‚é£æ ¼è§„èŒƒåŒ–å¯èƒ½æŒ‡çš„æ˜¯ä½¿**å­¦ç”Ÿç½‘ç»œåœ¨ä¸åŒçš„é£æ ¼è¾“å…¥ä¸‹ä¿æŒä¸€è‡´æ€§**ï¼Œè€Œ**ç»“æ„è§„èŒƒåŒ–åˆ™æ˜¯ç¡®ä¿å­¦ç”Ÿç½‘ç»œèƒ½å¤Ÿå­¦ä¹ åˆ°é‡è¦çš„ç»“æ„ä¿¡æ¯**ã€‚**ç‰©ç†å…ˆéªŒåˆ™æ˜¯åŸºäºå¯¹ä¸–ç•Œç‰©ç†è§„å¾‹çš„äº†è§£**ï¼Œä¾‹å¦‚åœ¨è§†è§‰ä»»åŠ¡ä¸­ï¼Œç‰©ä½“çš„å‡ ä½•å…³ç³»å’Œè¿åŠ¨çº¦æŸå¯ä»¥è¢«ç”¨ä½œæ­£åˆ™åŒ–é¡¹æ¥å¸®åŠ©ç½‘ç»œå­¦ä¹ æ›´åŠ çœŸå®å’Œæœ‰ç”¨çš„è¡¨ç¤ºã€‚

3. **Tang et al. [55] çš„é€æ­¥çŸ¥è¯†è’¸é¦å®ç°æ•°æ®æ— å…³é€‚åº”**ï¼š
   åœ¨Tangç­‰äººçš„ç ”ç©¶ä¸­ï¼Œä»–ä»¬é‡‡å–äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•æ¥å®ç°æ•°æ®æ— å…³é€‚åº”ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬é¦–å…ˆé€šè¿‡æ„å»ºçš„é‚»åŸŸå‡ ä½•ç»“æ„ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç„¶åä½¿ç”¨æœ€æ–°è½®æ¬¡å¾—åˆ°çš„ä¼ªæ ‡ç­¾æ¥æŒ‡å¯¼å½“å‰è®­ç»ƒè½®æ¬¡è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨æ²¡æœ‰ç›´æ¥ç›®æ ‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»å·²æœ‰çš„çŸ¥è¯†å’Œå‡ ä½•ç»“æ„ä¸­æ¨æ–­å‡ºç›®æ ‡æ•°æ®çš„å¯èƒ½ç»“æ„æ¥é€‚åº”æ–°çš„åŸŸã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯æ¸è¿›çš„ï¼Œæ„å‘³ç€éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ¨¡å‹é€šè¿‡ä¸æ–­çš„è‡ªæˆ‘æ”¹è¿›å’Œè‡ªæˆ‘è¯„ä¼°æ¥è°ƒæ•´è‡ªå·±çš„çŸ¥è¯†å’Œé¢„æµ‹ã€‚

æ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚ä¾‹å¦‚ï¼Œå¤šæ•™å¸ˆå¤šå­¦ç”Ÿæ¡†æ¶å¯ä»¥æä¾›å¤šæ ·åŒ–çš„å­¦ä¹ ä¿¡å·ï¼Œå¯èƒ½é€‚ç”¨äºæ ·æœ¬å¤šæ ·æ€§å¤§çš„ä»»åŠ¡ï¼›è€Œé£æ ¼å’Œç»“æ„è§„èŒƒåŒ–å¯èƒ½æ›´é€‚ç”¨äºéœ€è¦ç²¾ç»†åŒ–è§†è§‰è¯†åˆ«çš„ä»»åŠ¡ï¼›é€æ­¥çŸ¥è¯†è’¸é¦åˆ™å¯èƒ½é€‚åˆé‚£äº›å®Œå…¨æ²¡æœ‰ç›®æ ‡åŸŸæ•°æ®çš„æƒ…å†µã€‚

è¿™äº›æ–¹æ³•ä½“ç°äº†é¢†åŸŸé€‚åº”å’ŒçŸ¥è¯†è½¬ç§»ç ”ç©¶ä¸­çš„åˆ›æ–°æ€§å’Œå¤šæ ·æ€§ï¼Œæ¯ä¸€ç§æ–¹æ³•éƒ½è¯•å›¾ä»¥ä¸åŒçš„æ–¹å¼æ¥è§£å†³å¦‚ä½•åœ¨ä¸åŒçš„æ•°æ®é›†æˆ–åŸŸä¹‹é—´è¿ç§»å’Œé€‚åº”çŸ¥è¯†çš„é—®é¢˜ã€‚è¿™è¡¨æ˜åœ¨è®¾è®¡ä¸€ä¸ªé²æ£’çš„å­¦ä¹ ç³»ç»Ÿæ—¶ï¼Œè€ƒ



Y. Grandvalet and Y. Bengio, â€œSemisupervised learning by entropy
minimization,â€ Advances in Neural
Information Processing Systems, vol. 17,
2004.





#### 4.2.1 Entropy Minimization Loss

Most SFUDA methods utilize an entropy minimization lossÂ \[[194](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib194)\] to reduce uncertainty of model predictionsÂ \[[95](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib95), [195](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib195), [196](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib196), [197](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib197), [198](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib198), [199](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib199), [113](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib113), [112](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib112), [60](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib60), [76](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib76), [28](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib28), [200](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib200), [201](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib201)\]. This simple yet effective strategy encourages the model to generate one-hot predictions for more confident learning.



Entropy minimization is a concept utilized in semi-supervised learning to improve the learning from both labeled and unlabeled data by making the model's predictions as certain (or "confident") as possible. In the context of semi-supervised learning, labeled data is often scarce, so leveraging unlabeled data becomes crucial to enhance the model's performance.

**Entropy in Information Theory**: Entropy is a measure from information theory that quantifies the amount of uncertainty or unpredictability in the predictions of a model. In the context of classification, high entropy reflects a model's uncertainty about which class label to assign to a particular instance. If a model is very certain about its predictions, the entropy would be low. For a classifier, this would ideally be a "one-hot" distribution, where one class has a prediction probability of 1 and the rest have 0.

**Entropy Minimization**: The entropy minimization loss is designed to penalize high entropy predictions and reward low entropy, or more certain, predictions. The intuition is that, if the model is forced to make confident predictions (low entropy), it has to adjust its decision boundaries in a way that clusters the unlabeled data with the labeled instances of each class. This can often align with the true but unknown labels of the unlabeled data, thus improving the model's generalization ability.

**Entropy Minimization in Semi-Supervised Learning**: For semi-supervised learning, the entropy minimization loss can be particularly useful. With limited labeled data, the model may not learn a decision boundary that sufficiently separates the classes. By including the entropy minimization loss, the model can use the unlabeled data to better shape the decision boundary. Unlabeled instances are pushed towards the labeled instances of one class, helping to create more distinct clusters.

**How it Works**: The entropy minimization loss can be incorporated into the loss function of a model. During training, the model not only tries to minimize the prediction error on the labeled data (using standard loss functions like cross-entropy) but also minimizes the entropy of the predictions on the unlabeled data. The overall loss function would be a weighted sum of these two losses.

**Practical Considerations**: In practice, implementing entropy minimization requires careful tuning. If the weight of the entropy loss is too high, the model might make overly confident but incorrect predictions on the unlabeled data, leading to overfitting. Conversely, if the weight is too low, the unlabeled data may not contribute enough to the learning process.

**Conclusion**: Entropy minimization loss is a powerful tool in semi-supervised learning, enabling models to leverage unlabeled data effectively by reducing the uncertainty in the model's predictions. It encourages the model to make more decisive predictions on the unlabeled data, thus providing additional guidance for learning a better decision boundary.

#### 4.2.2 Diversity Enforcing Loss

To prevent predicted labels from collapsing to categories with larger number of samples, many studies leverage a diversity enforcing loss to encourage diverse predictions over target domainÂ \[[95](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib95), [202](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib202), [203](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib203), [81](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib81), [196](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib196), [204](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib204), [205](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib205), [206](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib206)\] . The usual practice is to maximize the entropy of empirical label distribution over the batch-wise average of model predictions.

The concept of a **Diversity Enforcing Loss** is introduced in the field of domain adaptation, particularly within methods that do not have access to source domain data, commonly referred to as Source-Free Unsupervised Domain Adaptation (SFUDA). This type of loss is designed to encourage the model to make diverse predictions across the target domain, especially when the target instances can be divided into sets that are similar or dissimilar to the source.

**Why Diversity Enforcing Loss?**
In a classification task, without enforcing diversity, a model may learn to predict majority classes over minority classes, especially when there is a class imbalance. This phenomenon is known as the model collapsing to the categories with a larger number of samples. Essentially, the model takes the easy way out by mostly predicting the dominant class, neglecting the smaller ones. This leads to poor generalization as the model fails to accurately learn and predict minority class instances.

**How Diversity Enforcing Loss Works:**
Diversity enforcing loss functions work against this by penalizing the model when it makes predictions that lack diversity. This type of loss encourages the model to distribute its predictions more evenly across all classes. In practice, it might involve terms in the loss function that encourage the model's output distribution to match a uniform distribution or the expected class distribution in the target domain.

For example, if a classifier is over-predicting a certain class on the target domain, the diversity loss would penalize this behavior, forcing the model to consider other classes as well. This can be particularly useful when dealing with datasets where some classes are underrepresented.

**Application in SFUDA:**
In the context of SFUDA, diversity enforcing loss helps in the following ways:

1. **Avoiding Biased Predictions**: It ensures that the model does not become biased towards the most frequent labels in the target domain, which is particularly important when source data is not available for reference.

2. **Better Generalization**: By encouraging predictions across a wide range of categories, the model is less likely to overfit to specific features of the target data that are not representative of the underlying task.

3. **Leveraging Unlabeled Data**: It is especially useful in semi-supervised settings where the model needs to learn from a large amount of unlabeled data in the target domain, as it promotes learning about all classes, not just the ones that are easiest to learn.

**Implementation Considerations:**

- **Balancing the Loss**: The diversity enforcing loss usually needs to be balanced with other loss functions (like entropy minimization loss or standard classification loss) to ensure that it does not overpower them, causing the model to ignore the actual data distribution.

- **Hyperparameter Tuning**: The weight of the diversity loss in the overall loss function is a hyperparameter that requires careful tuning, as it can significantly affect the model's performance.

In summary, a diversity enforcing loss is a strategy used in domain adaptation to prevent the model from becoming biased towards the more frequently occurring classes in the target domain. This is accomplished by penalizing predictions that lack diversity, thus encouraging the model to consider a broader range of classes during training.

#### 4.2.3 Label Smoothing Technique

In source-free adaptation studies, a pre-trained source model is generally obtained via training on labeled source data before adaptation stages. Currently, many studies use a label smoothing techniqueÂ \[[207](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib207), [208](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib208)\] to produce a robust source modelÂ \[[95](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib95), [102](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib102), [20](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib20), [31](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib31), [209](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib209), [210](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib210)\]. This technique aims to transform original training labels from hard labels (e.g., 1) to soft labels (e.g., 0.95), which prevents the source model from being over-confident, helping enhance its generalization ability. Also, the experiments have shown that label smoothing can encourage closer representations of training samples from the same categoryÂ \[[207](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib207)\]. With a more general and robust source model, it is likely to boost adaptation performance on target domain.The label smoothing technique is a regularization method used during the training of a model. It changes the way the model learns by adjusting the targets of the classification task. Let's break down the process:

**Traditional Training with Hard Labels:**
In conventional training of classification models, each training example is usually associated with a hard label. A hard label means that if, for example, you have three classes (cat, dog, and bird), the target output for a picture of a cat would be something like [1, 0, 0], indicating that the model should output 100% probability that the image is a cat and 0% for the other classes.

**Issues with Hard Labels:**
While effective, training on hard labels has its disadvantages. It can lead to overfitting, where the model performs well on the training data but fails to generalize to unseen data. Additionally, it can make the model overly confident in its predictions, sometimes to the detriment of its performance, particularly when the model encounters ambiguous examples or noise.

**Label Smoothing Technique:**
Label smoothing addresses these issues by softening the targets during training. Instead of hard labels, the model is trained on soft labels. Continuing with the above example, a soft label for a cat image might look something like [0.9, 0.05, 0.05], where the sum of the probabilities is still 1, but the model is no longer certain that the image is a cat. This encourages the model to be less confident in its predictions, which can lead to better generalization.

Here's how label smoothing works in more detail:

1. **Adjust the Target Distributions:**
   The true label for a class gets a value slightly less than 1, while the other classes get a small non-zero value, such that the probabilities sum to 1. For example, with three classes and a smoothing parameter \( \epsilon \) of 0.1, the target distribution for a cat image would change from [1, 0, 0] to [0.9, 0.05, 0.05].

2. **Regularization Effect:**
   By preventing the model from predicting the training samples with 100% certainty, label smoothing helps the model to generalize better. It also helps to mitigate issues related to overfitting.

3. **Implementation in Loss Function:**
   The model's loss function is then computed using these soft targets, which has the effect of penalizing overconfident predictions and leading to a more regularized and generalized model.

4. **Generalization to Unseen Data:**
   Label smoothing can help the model perform better on unseen data because it avoids the pitfalls of overconfidence and learns a more distributed representation of the data.

5. **Experiments and Evidence:**
   Experiments have shown that models trained with label smoothing tend to have closer representations of training samples from the same category, which is beneficial for generalization.

In summary, label smoothing is an approach to adjust the confidence of the model, making it more robust and improving its generalization capabilities by modifying the target labels used during training. It's particularly useful in transfer learning and domain adaptation, where the goal is to adapt a model to a new domain it hasn't seen during training.

#### 4.2.4 Model Regularization

Many regularization terms are utilized in existing SFUDA methods by incorporating some prior knowledge. For instance, an early learning regularizationÂ \[[40](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib40), [211](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib211), [121](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib121)\] is used to prevent the model from over-fitting to label noise. A stability regularizationÂ \[[39](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib39), [212](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib212), [213](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib213), [214](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib214)\] is leveraged to prevent parameters of the target model to deviate from those of the source model. A local smoothness regularizationÂ \[[39](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib39), [215](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib215)\] is used to encourage output consistency between the target model and its noise-perturbed counterpart, helping improve robustness of the target model. A mixup regularizationÂ \[[110](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib110), [31](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib31), [216](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib216), [115](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib115), [217](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib217)\] is used to enforce prediction consistency between original and augmented data, which can mitigate the negative influence of noisy labels.

Model regularization is a crucial aspect of improving the performance of deep learning models, especially in domain adaptation tasks where the model is trained on a source domain and is expected to perform well on a different target domain. Regularization techniques are used to prevent overfitting and to make the model more generalizable. Let's go through the four types of regularization techniques mentioned:

1. **Early Learning Regularization**:
   - This form of regularization is used to prevent the model from overfitting to noisy labels, which can occur when the model starts to memorize the noise instead of learning the underlying patterns.
   - The idea is to apply more weight to the regularization term at the beginning of the training process, gradually reducing its impact as training progresses.
   - The cited papers propose various approaches to implement this, such as adding a regularization term that discourages the model parameters from deviating too much from their initial values during the early stages of training.

2. **Stability Regularization**:
   - Stability regularization is used to ensure the parameters of the target model do not deviate significantly from those of the source model.
   - The goal is to preserve the knowledge acquired from the source domain while allowing the model to learn new representations from the target domain.
   - Techniques such as L2 regularization on the difference between the source and target model parameters are examples of this approach.

3. **Local Smoothness Regularization**:
   - This regularization encourages output consistency between the target model and its perturbed counterpart. The assumption is that small changes to the input should not drastically change the output.
   - It is usually implemented by penalizing the model if the output for an input and a slightly perturbed version of that input are different, thus enforcing the smoothness of the model's predictions.

4. **Mixup Regularization**:
   - Mixup regularization involves creating new training examples by combining images and labels from the training set in a convex manner.
   - This helps in making the model invariant to interpolations of the training samples and is particularly effective in preventing the model from being too confident and overfitting on noisy labels.
   - The regularization term enforces the prediction consistency between the original and the interpolated or augmented data.

Implementing these regularization techniques can help models become more robust and perform better when applied to new, unseen domains, as they encourage the models not to be overly confident and to generalize better from the training data to the target domain. Each technique introduces additional terms to the loss function that the model aims to minimize, thereby guiding the learning process to favor more generalizable patterns in the data.

#### 4.2.5 Confidence Thresholding

Many studies leverage pseudo-labeling to train the target model in a self-supervised way. Instead of utilizing a manually-designed threshold to identify reliable/confident pseudo-labels, a commonly used strategy is automatically learning the confidence threshold for reliable pseudo-label selectionÂ \[[218](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib218)\]. To further tackle the class-imbalance problem, some studiesÂ \[[213](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib213), [219](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib219), [76](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib76), [79](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib79), [220](https://ar5iv.labs.arxiv.org/html/2301.00265?_immersive_translate_auto_translate=0#bib.bib220)\] propose to learn dynamic threshold for each category, which provides a fair chance for categories with limited samples to generate pseudo-labels for self-training.



Confidence thresholding in the context of self-supervised learning and domain adaptation is a strategy used to filter and select the most reliable pseudo-labels generated by the model. Pseudo-labeling is a technique where a model uses its own predictions to continue learning in the absence of labels. However, this can lead to noise and errors propagating through the model, especially when the confidence in these pseudo-labels is not managed effectively. Confidence thresholding is utilized to mitigate this issue.

**Confidence Threshold for Reliable Pseudo-label Selection**:
Initially, a confidence threshold is a predefined or dynamically adjusted parameter that determines how confident the model should be in its pseudo-label predictions to consider them for further training. For instance, if the confidence threshold is set to 0.9, only predictions with a confidence score above 90% will be accepted as pseudo-labels.

This threshold can be fixed, where a single value is chosen heuristically or through cross-validation. However, this method does not adapt to the varying difficulty of predicting different classes and can be inefficient, especially in scenarios with class imbalance.

**Learning Dynamic Threshold for Each Category**:
To address the limitations of a fixed threshold, dynamic thresholding learns a unique threshold for each category based on the model's performance and the data distribution. This allows categories with fewer samples or more difficult classification boundaries to have a fair chance of contributing to the model's learning.

The dynamic threshold is often learned by observing the model's prediction scores on the target data and adjusting the threshold per category to maximize some performance metric, such as accuracy or F1-score, on validation data. This approach can be more flexible and better suited for datasets with class imbalance or varying levels of difficulty across classes.

Implementing Confidence Thresholding in Domain Adaptation:
In domain adaptation, especially when the source data is not accessible (Source-Free Unsupervised Domain Adaptation - SFUDA), models are trained using only target data. Here, confidence thresholding is crucial to ensure that the model does not reinforce its mistakes during self-training. By learning confidence thresholds that adapt to each category, the model can improve its predictions iteratively and avoid overfitting to potentially noisy pseudo-labels.

The process typically involves the following steps:

1. **Pseudo-label Generation**: The target model predicts labels for the target domain data.
2. **Confidence Assessment**: For each prediction, the model assesses the confidence of its prediction.
3. **Threshold Application**: Predictions with confidence scores above the learned threshold for their respective category are retained as pseudo-labels.
4. **Model Updating**: The model is updated (trained) using the selected pseudo-labels, improving its predictions iteratively.

The confidence thresholding strategy is critical in SFUDA methods to ensure quality control over the pseudo-labels used for further training the model. It helps in maintaining a balance between exploring the target domain's data distribution and exploiting the learned knowledge without supervision from the source domain. The goal is to enhance the model's generalization ability to new, unseen target data by focusing on high-confidence predictions and reducing the impact of noisy labels.